{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional, Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", sep='|')\n",
    "test = pd.read_csv(\"test.csv\", sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25000\n",
       "0    15000\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Entertaining enough for those who don't think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I bought it yesterday havent started watching ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This movie tells the story of three kids who g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>You wanna know what its like for a Black perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Warner Archive has finally released an epic fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText\n",
       "0        0  Entertaining enough for those who don't think ...\n",
       "1        1  I bought it yesterday havent started watching ...\n",
       "2        1  This movie tells the story of three kids who g...\n",
       "3        1  You wanna know what its like for a Black perso...\n",
       "4        1  Warner Archive has finally released an epic fi..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = './'\n",
    "TEXT_DATA_FILE = 'train.csv'\n",
    "HEADER = True\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.asarray(train['overall'], dtype='int8')\n",
    "data = np.asarray(train['reviewText'])\n",
    "data_test = np.asarray(test['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spliting original data on train and validation sets\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(data, labels,\n",
    "                     test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000,) (36000,)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data shape: [(4000,), (4000,)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation data shape: {}\".format([data_val.shape, labels_val.shape]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      " I came into this movie expecting the years greatest hit, like so many people told me.  You would need to pay me, pretty well, to see this movie again.  It's like they stole they plot from \"Dude Where's My Car\" and replaced the car with a guy.  The humor was extremely juvenile and repetitive.I understand it's supposed to be a funny movie, I enjoy most Will Ferrell movies for example, but this movie wasn't even funny.  I can't even say it had a few funny parts, it had maybe one, two tops.I kept waiting for the \"movie of the year\" type scenes and they never came.Simply awful.\n",
      "Sentence in indexes:\n",
      " [8, 335, 79, 9, 16, 1115, 1, 147, 734, 629, 33, 30, 108, 97, 595, 71, 19, 56, 346, 5, 875, 71, 202, 64, 5, 70, 9, 16, 157, 46, 33, 29, 4314, 29, 142, 34, 3267, 6519, 48, 648, 2, 2533, 1, 648, 14, 3, 292, 1, 374, 12, 714, 5384, 2, 4406, 8, 420, 46, 542, 5, 26, 3, 175, 16, 8, 239, 88, 61, 5965, 85, 13, 612, 17, 9, 16, 313, 62, 175, 8, 191, 62, 139, 10, 65, 3, 163, 175, 513, 10, 65, 333, 24, 94, 6520, 8, 704, 927, 13, 1, 16, 4, 1, 279, 551, 138, 2, 29, 114, 335, 378, 640]\n",
      "Sentence fitted to max length:\n",
      " [ 612   17    9   16  313   62  175    8  191   62  139   10   65    3  163\n",
      "  175  513   10   65  333   24   94 6520    8  704  927   13    1   16    4\n",
      "    1  279  551  138    2   29  114  335  378  640]\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary size and maximum sentence length\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "print(\"Original sentence:\\n\", data_train[0])\n",
    "\n",
    "# create a dictionary with Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='#$%&()*+-/:;<=>@[\\\\]^{|}~\\t\\n,.!\"')\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "\n",
    "# replacing words with their indexes from our dictionary\n",
    "X_train = tokenizer.texts_to_sequences(data_train)\n",
    "X_val = tokenizer.texts_to_sequences(data_val)\n",
    "\n",
    "print(\"Sentence in indexes:\\n\", X_train[0])\n",
    "\n",
    "# fit each sentence to max length\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Sentence fitted to max length:\\n\", X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(data_test)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 40)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with vector representation: 400000\n"
     ]
    }
   ],
   "source": [
    "# path to embeddings file\n",
    "EMBEDDINGS_DIR = '../embeddings'\n",
    "EMBEDDINGS_FILE = 'glove.6B.50d.txt'\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "first_10000 = {k: v for k, v in tokenizer.word_index.items() if v < 10000}\n",
    "\n",
    "# upload embeddings\n",
    "embeddings = {}\n",
    "with zipfile.ZipFile(os.path.join(EMBEDDINGS_DIR, EMBEDDINGS_FILE+'.zip')) as myzip:\n",
    "    with myzip.open(EMBEDDINGS_FILE) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0].decode('UTF-8')\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        del values, word, coefs, line\n",
    "print(\"Number of words with vector representation:\", len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embeddings matrix where each row is word index\n",
    "\n",
    "embedding_matrix = np.zeros((tokenizer.num_words, EMBEDDING_DIM))\n",
    "for word, i in first_10000.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               120800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 621,001\n",
      "Trainable params: 121,001\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NAME = \"bidirectional_lstm\"\n",
    "\n",
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)\n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop training model if accuracy does not increase more than five epochs\n",
    "callback_1 = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "# best model saving\n",
    "callback_2 = ModelCheckpoint(\"../models/model_{}.hdf5\".format(NAME), monitor='val_acc',\n",
    "                                 save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.6344 - acc: 0.6375Epoch 00000: val_acc improved from -inf to 0.68250, saving model to ../models/model_bidirectional_lstm.hdf5\n",
      "36000/36000 [==============================] - 75s - loss: 0.6342 - acc: 0.6379 - val_loss: 0.6038 - val_acc: 0.6825\n",
      "Epoch 2/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.5676 - acc: 0.6970Epoch 00001: val_acc improved from 0.68250 to 0.72600, saving model to ../models/model_bidirectional_lstm.hdf5\n",
      "36000/36000 [==============================] - 102s - loss: 0.5673 - acc: 0.6972 - val_loss: 0.5329 - val_acc: 0.7260\n",
      "Epoch 3/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7215Epoch 00002: val_acc improved from 0.72600 to 0.73125, saving model to ../models/model_bidirectional_lstm.hdf5\n",
      "36000/36000 [==============================] - 65s - loss: 0.5410 - acc: 0.7216 - val_loss: 0.5207 - val_acc: 0.7312\n",
      "Epoch 4/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7288Epoch 00003: val_acc improved from 0.73125 to 0.73725, saving model to ../models/model_bidirectional_lstm.hdf5\n",
      "36000/36000 [==============================] - 67s - loss: 0.5286 - acc: 0.7289 - val_loss: 0.5027 - val_acc: 0.7372\n",
      "Epoch 5/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.5165 - acc: 0.7357Epoch 00004: val_acc improved from 0.73725 to 0.74700, saving model to ../models/model_bidirectional_lstm.hdf5\n",
      "36000/36000 [==============================] - 68s - loss: 0.5164 - acc: 0.7358 - val_loss: 0.4920 - val_acc: 0.7470\n",
      "Epoch 6/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.7445Epoch 00005: val_acc improved from 0.74700 to 0.75275, saving model to ../models/model_bidirectional_lstm.hdf5\n",
      "36000/36000 [==============================] - 68s - loss: 0.5077 - acc: 0.7443 - val_loss: 0.4960 - val_acc: 0.7527\n",
      "Epoch 7/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.4939 - acc: 0.7558Epoch 00006: val_acc improved from 0.75275 to 0.76325, saving model to ../models/model_bidirectional_lstm.hdf5\n",
      "36000/36000 [==============================] - 66s - loss: 0.4940 - acc: 0.7557 - val_loss: 0.4727 - val_acc: 0.7632\n",
      "Epoch 8/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.4846 - acc: 0.7600Epoch 00007: val_acc improved from 0.76325 to 0.77450, saving model to ../models/model_bidirectional_lstm.hdf5\n",
      "36000/36000 [==============================] - 64s - loss: 0.4846 - acc: 0.7602 - val_loss: 0.4657 - val_acc: 0.7745\n",
      "Epoch 9/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.4820 - acc: 0.7626Epoch 00008: val_acc did not improve\n",
      "36000/36000 [==============================] - 63s - loss: 0.4820 - acc: 0.7626 - val_loss: 0.4607 - val_acc: 0.7695\n",
      "Epoch 10/10\n",
      "35840/36000 [============================>.] - ETA: 0s - loss: 0.4697 - acc: 0.7710Epoch 00009: val_acc improved from 0.77450 to 0.77675, saving model to ../models/model_bidirectional_lstm.hdf5\n",
      "36000/36000 [==============================] - 64s - loss: 0.4696 - acc: 0.7711 - val_loss: 0.4582 - val_acc: 0.7767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2801cbd30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, labels_train, validation_data=[X_val, labels_val], \n",
    "          batch_size=1024, epochs=10, callbacks=[callback_1, callback_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22464/22500 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71437711],\n",
       "       [ 0.10330234],\n",
       "       [ 0.31465235]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:3]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}